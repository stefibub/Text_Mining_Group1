{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.3 Sentiment Analysis with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this notebook is on performing sentiment analysis using the scikit-learn package. Material from [this notebook](http://www.pitt.edu/~naraehan/presentation/Movie+Reviews+sentiment+analysis+with+Scikit-Learn.html) was used.\n",
    "\n",
    "**At the end of this notebook, you will be able to**:\n",
    "* load the training data, i.e., the movie reviews\n",
    "* inspect the training data, i.e., the movie reviews\n",
    "* extracting features from the training data\n",
    "* training and evaluating the *NaiveBayesClassifier*\n",
    "* apply the classifier to fake movie reviews\n",
    "\n",
    "**If you want to learn more, you might find the following link useful:**\n",
    "* [documentation on dataset loading](http://scikit-learn.org/stable/datasets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a machine learning system we need a number of packages, the most important ones are *sklearn* and *numpy* to manipulate out data and call machine learning functions. Since we are dealing with texts, we also need some specific packages from *sklearn* to operate on texts to get words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "We are first going to load and inspect the **airlinetweets** dataset (which is included in the zip file you downloaded from Github). We are going to use the method **load_files** as part of sklearn.\n",
    "Let's first inspect what the help message of the function **load_files** states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_files in module sklearn.datasets._base:\n",
      "\n",
      "load_files(container_path, *, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0, allowed_extensions=None)\n",
      "    Load text files with categories as subfolder names.\n",
      "    \n",
      "    Individual samples are assumed to be files stored a two levels folder\n",
      "    structure such as the following:\n",
      "    \n",
      "        container_folder/\n",
      "            category_1_folder/\n",
      "                file_1.txt\n",
      "                file_2.txt\n",
      "                ...\n",
      "                file_42.txt\n",
      "            category_2_folder/\n",
      "                file_43.txt\n",
      "                file_44.txt\n",
      "                ...\n",
      "    \n",
      "    The folder names are used as supervised signal label names. The individual\n",
      "    file names are not important.\n",
      "    \n",
      "    This function does not try to extract features into a numpy array or scipy\n",
      "    sparse matrix. In addition, if load_content is false it does not try to\n",
      "    load the files in memory.\n",
      "    \n",
      "    To use text files in a scikit-learn classification or clustering algorithm,\n",
      "    you will need to use the :mod:`~sklearn.feature_extraction.text` module to\n",
      "    build a feature extraction transformer that suits your problem.\n",
      "    \n",
      "    If you set load_content=True, you should also specify the encoding of the\n",
      "    text using the 'encoding' parameter. For many modern text files, 'utf-8'\n",
      "    will be the correct encoding. If you leave encoding equal to None, then the\n",
      "    content will be made of bytes instead of Unicode, and you will not be able\n",
      "    to use most functions in :mod:`~sklearn.feature_extraction.text`.\n",
      "    \n",
      "    Similar feature extractors should be built for other kind of unstructured\n",
      "    data input such as images, audio, video, ...\n",
      "    \n",
      "    If you want files with a specific file extension (e.g. `.txt`) then you\n",
      "    can pass a list of those file extensions to `allowed_extensions`.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <datasets>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    container_path : str\n",
      "        Path to the main folder holding one subfolder per category.\n",
      "    \n",
      "    description : str, default=None\n",
      "        A paragraph describing the characteristic of the dataset: its source,\n",
      "        reference, etc.\n",
      "    \n",
      "    categories : list of str, default=None\n",
      "        If None (default), load all the categories. If not None, list of\n",
      "        category names to load (other categories ignored).\n",
      "    \n",
      "    load_content : bool, default=True\n",
      "        Whether to load or not the content of the different files. If true a\n",
      "        'data' attribute containing the text information is present in the data\n",
      "        structure returned. If not, a filenames attribute gives the path to the\n",
      "        files.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data: might be important for models that\n",
      "        make the assumption that the samples are independent and identically\n",
      "        distributed (i.i.d.), such as stochastic gradient descent.\n",
      "    \n",
      "    encoding : str, default=None\n",
      "        If None, do not try to decode the content of the files (e.g. for images\n",
      "        or other non-text content). If not None, encoding to use to decode text\n",
      "        files to Unicode if load_content is True.\n",
      "    \n",
      "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "        Instruction on what to do if a byte sequence is given to analyze that\n",
      "        contains characters not of the given `encoding`. Passed as keyword\n",
      "        argument 'errors' to bytes.decode.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=0\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    allowed_extensions : list of str, default=None\n",
      "        List of desired file extensions to filter the files to be loaded.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : list of str\n",
      "            Only present when `load_content=True`.\n",
      "            The raw text data to learn.\n",
      "        target : ndarray\n",
      "            The target labels (integer index).\n",
      "        target_names : list\n",
      "            The names of target classes.\n",
      "        DESCR : str\n",
      "            The full description of the dataset.\n",
      "        filenames: ndarray\n",
      "            The filenames holding the dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the function requires the following structure in order for it to work:\n",
    "* container_folder/\n",
    "    * category_1_folder/ (e.g., 'pos')\n",
    "        * file_1.txt\n",
    "        * file_2.txt\n",
    "        * ...\n",
    "        file_42.txt\n",
    "    * category_2_folder/ (e.g., 'neg')\n",
    "        * file_43.txt\n",
    "        * file_44.txt\n",
    "        * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether our **airlinetweets** corpus has this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/pc/Documents/GitHub/Text_Mining_Group1/Lab3/airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "print('path:', airline_tweets_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      airline_tweets_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pc/Documents/GitHub/Text_Mining_Group1/Lab3/airlinetweets'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(airline_tweets_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect whether the corpus has the required structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, it is! Let's now load it using the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all files as training data.\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many files do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4755"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airline_tweets_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target names (\"classes\") are automatically generated from subfolder names\n",
    "airline_tweets_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not agree with these labels, you could change the names of the subdirectories. If you do not agree with the distinctions, you could add other folders with other category names and move files to these folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many do we have for each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral 1515\n",
      "positive 1490\n",
      "negative 1750\n"
     ]
    }
   ],
   "source": [
    "freqs = Counter(airline_tweets_train.target)\n",
    "for category, frequency in freqs.items():\n",
    "    print(airline_tweets_train.target_names[category], frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'@AmericanAir Why is your cover photo of TWA? Just wondering.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's inspect the first file\n",
    "airline_tweets_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pc/Documents/GitHub/Text_Mining_Group1/Lab3/airlinetweets/neutral/AL_570069345818161152.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first file is in \"neutral\" folder\n",
    "airline_tweets_train.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'airline_tweets_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# first file is a neutral review and is mapped to index 1 in target_names\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m airline_tweets_train\u001b[38;5;241m.\u001b[39mtarget[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'airline_tweets_train' is not defined"
     ]
    }
   ],
   "source": [
    "# first file is a neutral review and is mapped to index 1 in target_names\n",
    "airline_tweets_train.target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out what the index means by inserting it into **target_names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'airline_tweets_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m airline_tweets_train\u001b[38;5;241m.\u001b[39mtarget_names[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'airline_tweets_train' is not defined"
     ]
    }
   ],
   "source": [
    "airline_tweets_train.target_names[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from the previous labs that some of the NLTK data is structured precisely in the same way as the ailinetweets data, e.g. nltk_data/corpora/movie_reviews.\n",
    "\n",
    "Likewise, we can load this datat in the same way using the load_files function. First adapt the path below to point to the location on your local laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movie_reviews_train = load_files(str(movie_reviews_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/pc/nltk_data/corpora/movie_reviews\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_folder = cwd.joinpath('/Users/pc/nltk_data/corpora/movie_reviews')\n",
    "print('path:', movie_reviews_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      movie_reviews_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_reviews_train.data))\n",
    "print(movie_reviews_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from training data (see notebook Lab2.3 Feature representation.ipynb for more information)\n",
    "Note: you might get a warning when you run the following cell. You do NOT have to resolve the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize airline object, and then turn airline tweets train data into a vector \n",
    "\n",
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to represent each document in terms of this vector, we use the *fit_transform* function to generate a matrix of documents (the rows) and the vectors with the scores for each words that occurs in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pc/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/pc/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created a vector representation *airline_vec* of the complete vocabulary of the full data set. Every position in this vector represents a unique word token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2907\n"
     ]
    }
   ],
   "source": [
    "#Total number of word features or the length of the total vector\n",
    "print(len(airline_vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'various\", \"'ve\", '(', ')', '*', '+', '+1', '-', '--', '-/', '.', '..', '...', '....', '.....', '.50', '/', '//t.co/8wbzorrn3c', '//t.co/aqjn4hwnac', '//t.co/f2lfulcbq7', '0', '1', '1.', '1.5', '1.75', '1/2', '10', '10-24', '100', '100+', '1000', '103', '1051', '1071', '10:30', '10:55', '11', '1142']\n"
     ]
    }
   ],
   "source": [
    "# First 50 feature names\n",
    "print(list(airline_vec.get_feature_names_out())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1953"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'plane' is found in the corpus, mapped to index 1948\n",
    "airline_vec.vocabulary_.get('plane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the dimensions of our feature array by getting the spape: the rows (documents) and columns (the word vector length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4755, 2907)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# large dimensions! 4,755 documents, 2902 unique terms. \n",
    "airline_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the matrix to an array and get the first element and look at the vector values for slots 100 till 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(airline_counts.toarray()[0][100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most values are zero's and just a few have the value 1. This is what we call a sparse vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous Lab, we can also transform the counts into information value scores using the *TfidfTransformer* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the shape remains the same but the values are now scores between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4755, 2907)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19103151\n",
      " 0.07301622 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Same dimensions, now with tf-idf values instead of raw frequency counts\n",
    "print(airline_tfidf.shape)\n",
    "print(airline_tfidf.toarray()[0][100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a Naive Bayes classifier\n",
    "\n",
    "We can now use the above data representation as training data to build a classifier. the Sklearn package already associated each row (a document) in our data represenation with a label by taking the name of the data subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets_train.target_names[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a simple Naive Bayes classifier to train a model. Because we have multiple labels (negative, positive, neutral), we need a multinomial classifier as we are dealing with 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ready to build a classifier. \n",
    "# We will use Multinominal Naive Bayes as our model\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy for machine learning package to read the above vector representations and associated these with any type of label. However, we also want to test the data. For that purpose, we need to exclude part of the data from a training set.\n",
    "\n",
    "To train the classifier, we will first split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# from sklearn.cross_validation import train_test_split  # deprecated in 0.18\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose 80% training and 20% test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One instance looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17509243, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train[55].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's label is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we know is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets_train.target_names[y_train[55]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *fit* function of sklearn takes as input the training data and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the classifier, we can apply it to test data that is represented in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "y_pred = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one tweet review: b'@AmericanAir Why is your cover photo of TWA? Just wondering.'\n",
      "gold label: 1\n",
      "classifier predicted: 0\n"
     ]
    }
   ],
   "source": [
    "print('one tweet review:', airline_tweets_train.data[0])\n",
    "print('gold label:', airline_tweets_train.target[0])\n",
    "print('classifier predicted:', y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides functions to obtain the recall, precision and f-measure for the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8170347003154574"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(y_true=y_test,\n",
    "                             y_pred=y_pred,\n",
    "                             average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the least and most important features per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 160.09275630383053 united\n",
      "0 114.41944153604705 .\n",
      "0 97.43116570311382 @\n",
      "0 94.90390115327065 ``\n",
      "0 56.70056096810854 flight\n",
      "0 50.656621623539685 ?\n",
      "0 45.17288649627717 #\n",
      "0 41.75223023015559 !\n",
      "0 38.140201635782425 n't\n",
      "0 30.58415369067599 ''\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 104.15883081351095 @\n",
      "1 86.66497134614868 ?\n",
      "1 60.449468684909334 jetblue\n",
      "1 58.92776730072946 southwestair\n",
      "1 57.757178681606455 .\n",
      "1 53.53928169176665 ``\n",
      "1 53.124374216174694 :\n",
      "1 50.07716965084536 americanair\n",
      "1 40.966695040518616 usairways\n",
      "1 38.614261275361486 #\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 170.52995754138132 !\n",
      "2 103.23150933655478 @\n",
      "2 84.20024698618586 .\n",
      "2 80.13187099856725 thanks\n",
      "2 79.23767046049426 thank\n",
      "2 64.06183938257955 southwestair\n",
      "2 62.110415448952985 jetblue\n",
      "2 60.647532666637105 ``\n",
      "2 52.49960924435237 americanair\n",
      "2 49.70150072117376 #\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=10): #n is the number of top features\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying classifier on our own data\n",
    "Now we can apply our classifier to new data.\n",
    "In the example below, these are movie reviews. In the exercise, you will choose tweets that you've selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# very short and fake movie reviews\n",
    "reviews_new = ['This movie was excellent', \n",
    "               'Absolute joy ride', \n",
    "               'Steven Seagal was terrible', \n",
    "               'Steven Seagal shined through.', \n",
    "               'This was certainly a movie', \n",
    "               'Two thumbs up', \n",
    "               'I fell asleep halfway through', \n",
    "               \"We can't wait for the sequel!!\", \n",
    "               'I cannot recommend this highly enough', \n",
    "               'instant classic.', \n",
    "               'Steven Seagal was amazing.']\n",
    "len(reviews_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to apply our model to these text, we need to represent the text using the same vectors as we used for training the model.\n",
    "The sklearn transformer function does this work for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2907)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We re-use airline_vec to transform it in the same way as the training data\n",
    "new_counts = airline_vec.transform(reviews_new)\n",
    "new_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that words in our movie reviews that are NOT in the training data, will not be represented as there are no slots in the vectors from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute tf idf values\n",
    "reviews_new_tfidf = tfidf_transformer.transform(new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2907)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_new_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "pred = clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was excellent => positive\n",
      "Absolute joy ride => positive\n",
      "Steven Seagal was terrible => negative\n",
      "Steven Seagal shined through. => negative\n",
      "This was certainly a movie => negative\n",
      "Two thumbs up => negative\n",
      "I fell asleep halfway through => positive\n",
      "We can't wait for the sequel!! => negative\n",
      "I cannot recommend this highly enough => negative\n",
      "instant classic. => negative\n",
      "Steven Seagal was amazing. => positive\n"
     ]
    }
   ],
   "source": [
    "# print out results ()\n",
    "for review, predicted_label in zip(reviews_new, pred):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        airline_tweets_train.target_names[predicted_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier with movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we loaded the NLTK movie review data set using the sklearn function *load_files* in the same way as we have done for the ailinetweets. This means we can build a simple classifier from this data as well and apply it to the same set of review_news and compare the two systems.\n",
    "\n",
    "We proceed in three simple steps:\n",
    "\n",
    "<ol>\n",
    "    <li>We create a CountVectorizer to vectorize the training texts based on the total vocabulary using the *fit_transform* function\n",
    "    <li>We transfer the values using the tfidf_transformer into information values\n",
    "    <li>We create a *MultinomialNB* classifier from the vectorised documents and their labels\n",
    "</ol>\n",
    "\n",
    "Once we trained the classifier, we can apply it to the above examples by transforming these to repsentations that are compatible to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pc/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/pc/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "movie_counts = movie_vec.fit_transform(movie_reviews_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_tfidf = tfidf_transformer.fit_transform(movie_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn load_files functions not only return the data for training but also the classification labels that go with it based in the folder names that contain the text files. The *target* attribute contains the lists of values as integer indexes corresponding to each document representation in the data and *target_names* contain the meaning of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "[0 1 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews_train.target_names)\n",
    "# print the labels for the first ten documents\n",
    "print(movie_reviews_train.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(movie_tfidf, movie_reviews_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 25139)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We re-use airline_vec to transform it in the same way as the training data\n",
    "new_counts = movie_vec.transform(reviews_new)\n",
    "new_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_new_tfidf = tfidf_transformer.transform(new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was excellent => pos\n",
      "Absolute joy ride => pos\n",
      "Steven Seagal was terrible => neg\n",
      "Steven Seagal shined through. => neg\n",
      "This was certainly a movie => neg\n",
      "Two thumbs up => neg\n",
      "I fell asleep halfway through => neg\n",
      "We can't wait for the sequel!! => neg\n",
      "I cannot recommend this highly enough => pos\n",
      "instant classic. => pos\n",
      "Steven Seagal was amazing. => neg\n"
     ]
    }
   ],
   "source": [
    "# print out results ()\n",
    "for review, predicted_label in zip(reviews_new, pred):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        movie_reviews_train.target_names[predicted_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
