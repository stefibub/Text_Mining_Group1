{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/stefaniaconte/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "{'words': 'EU', 'pos': 'NNP'}\n",
      "{'words': 'rejects', 'pos': 'VBZ'}\n",
      "{'words': 'German', 'pos': 'JJ'}\n",
      "{'words': 'call', 'pos': 'NN'}\n",
      "{'words': 'to', 'pos': 'TO'}\n",
      "\n",
      "Training Gold Labels:\n",
      "B-ORG\n",
      "O\n",
      "B-MISC\n",
      "O\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "# Prepare the training features and labels\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "   \n",
    "    a_dict = {\n",
    "        'words': token,\n",
    "        'pos': pos,\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "\n",
    "\n",
    "#debugging by priting till certain n  \n",
    "print(\"Training Features:\")\n",
    "for i in range(min(5, len(training_features))):  \n",
    "    print(training_features[i])\n",
    "\n",
    "print(\"\\nTraining Gold Labels:\")\n",
    "for i in range(min(5, len(training_gold_labels))):  \n",
    "    print(training_gold_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Features:\n",
      "{'words': 'SOCCER', 'pos': 'NN'}\n",
      "{'words': '-', 'pos': ':'}\n",
      "{'words': 'JAPAN', 'pos': 'NNP'}\n",
      "{'words': 'GET', 'pos': 'VB'}\n",
      "{'words': 'LUCKY', 'pos': 'NNP'}\n",
      "\n",
      "Test Gold Labels:\n",
      "O\n",
      "O\n",
      "B-LOC\n",
      "O\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        'words': token,\n",
    "        'pos': pos,\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)\n",
    "\n",
    "#debugging by priting \n",
    "print(\"Test Features:\")\n",
    "for i in range(min(5, len(test_features))):  \n",
    "    print(test_features[i])\n",
    "\n",
    "print(\"\\nTest Gold Labels:\")\n",
    "for i in range(min(5, len(test_gold_labels))):  \n",
    "    print(test_gold_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set contains 203621 instances.\n",
      "Test set contains 46435 instances.\n",
      "\n",
      "Frequency of each NERC label in the training data:\n",
      "  B-ORG: appears 6321 times\n",
      "  O: appears 169578 times\n",
      "  B-MISC: appears 3438 times\n",
      "  B-PER: appears 6600 times\n",
      "  I-PER: appears 4528 times\n",
      "  B-LOC: appears 7140 times\n",
      "  I-ORG: appears 3704 times\n",
      "  I-MISC: appears 1155 times\n",
      "  I-LOC: appears 1157 times\n",
      "\n",
      "Distribution of NERC labels in the testing data:\n",
      "  O: appears 38323 times\n",
      "  B-LOC: appears 1668 times\n",
      "  B-PER: appears 1617 times\n",
      "  I-PER: appears 1156 times\n",
      "  I-LOC: appears 257 times\n",
      "  B-MISC: appears 702 times\n",
      "  I-MISC: appears 216 times\n",
      "  B-ORG: appears 1661 times\n",
      "  I-ORG: appears 835 times\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sample_list = [1, 2, 1, 3, 2, 5]\n",
    "frequency_counter = Counter(sample_list)\n",
    "\n",
    "#Calculating size of training and testing sets\n",
    "total_training = len(training_features)\n",
    "total_testing = len(test_features)\n",
    "\n",
    "#Occurrences of each NERC label \n",
    "frequency_train_labels = Counter(training_gold_labels)\n",
    "frequency_test_labels = Counter(test_gold_labels)\n",
    "\n",
    "#Instances in the datasets\n",
    "print(f\"Training set contains {total_training} instances.\")\n",
    "print(f\"Test set contains {total_testing} instances.\\n\")\n",
    "\n",
    "#Frequency of NERC label in training data\n",
    "print(\"Frequency of each NERC label in the training data:\")\n",
    "for entity, count in frequency_train_labels.items():\n",
    "    print(f\"  {entity}: appears {count} times\")\n",
    "\n",
    "#Frequency of NERC label in testing data\n",
    "print(\"\\nDistribution of NERC labels in the testing data:\")\n",
    "for entity, count in frequency_test_labels.items():\n",
    "    print(f\"  {entity}: appears {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contains 203621 instances, while the test data contains 46435 instances. This indicates a sizeable amount of data for training, which is beneficial for building a robust model. \n",
    "The dataset demonstrates significant imbalances among the NERC labels. In particular, the 'O' label, which represents tokens outside of named entities, is far more prevalent than other labels in both the training and testing datasets. This reflects a common situation in NERC tasks, where the bulk of text tokens are not part of named entities.\n",
    "\n",
    "In the analysis of label distribution across both datasets, while the relative proportions of labels maintain a degree of consistency, there are differences in actual numbers due to the varying sizes of the datasets. Specifically, labels associated with locations ('I-LOC') and miscellaneous entities ('I-MISC') show noticeable discrepancies, especially with 'I-LOC' being less represented in the test data than in the training data.\n",
    "\n",
    "Going deeper, we notice that :\n",
    "\n",
    "- The dominance of the 'O' label in these datasets is in line with expectations, as it categorizes words not identified as part of named entities.\n",
    "- The presence of specific named entity labels (such as B-ORG, B-MISC, B-PER, B-LOC, I-PER, I-ORG, I-MISC, I-LOC) is considerably less than the 'O' label.\n",
    "- Comparatively, there are more instances of each specific label in the training data than in the test data, relative to their total counts.\n",
    "- Labels for locations (B-LOC, I-LOC) are seen more frequently than those for miscellaneous categories (B-MISC, I-MISC).\n",
    "- There is a higher occurrence of 'Beginning' labels (B-) over 'Inside' labels (I-), which aligns with the structure of named entities beginning with a 'B-' label followed by any 'I-' labels.\n",
    "- The distribution of different entity types (PER, ORG, LOC, MISC) is uneven, with personal and location entities more common than organizational and miscellaneous ones.\n",
    "\n",
    "A balanced dataset in NERC would exhibit an equitable distribution of instances across all entity and non-entity categories. However, such balance is uncommon due to the natural predominance of certain types of entities in language, leading to the observed disparities in this dataset and potentially impacting the effectiveness of trained NERC systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1.post1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of vectorized training data: (203621, 27361)\n",
      "Dimensions of vectorized testing data: (46435, 27361)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Concatenate train and test features\n",
    "combined_features = training_features + test_features\n",
    "\n",
    "#Vectorization to our combined feature collection\n",
    "dict_vectorizer = DictVectorizer()\n",
    "features_vectorized = dict_vectorizer.fit_transform(combined_features)\n",
    "\n",
    "#Divide the vectorized features back into separate training and testing sets\n",
    "total_train_samples = len(training_features)\n",
    "vectorized_train_set = features_vectorized[:total_train_samples]\n",
    "vectorized_test_set = features_vectorized[total_train_samples:]\n",
    "\n",
    "\n",
    "print(\"Dimensions of vectorized training data:\", vectorized_train_set.shape)\n",
    "print(\"Dimensions of vectorized testing data:\", vectorized_test_set.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       B-ORG       0.79      0.52      0.63      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.70      0.47      0.56       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "\n",
    "lin_clf.fit(vectorized_train_set, training_gold_labels)\n",
    "test_predictions = lin_clf.predict(vectorized_test_set)\n",
    "report = classification_report(test_gold_labels, test_predictions)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 38312, 'I-PER': 3028, 'B-LOC': 1592, 'B-ORG': 1088, 'B-PER': 821, 'B-MISC': 596, 'I-ORG': 555, 'I-MISC': 223, 'I-LOC': 220})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.08      0.05      0.06      1668\n",
      "      B-MISC       0.04      0.04      0.04       702\n",
      "       B-ORG       0.18      0.01      0.02      1661\n",
      "       B-PER       0.08      0.01      0.02      1617\n",
      "       I-LOC       0.04      0.19      0.06       257\n",
      "      I-MISC       0.06      0.28      0.10       216\n",
      "       I-ORG       0.06      0.01      0.01       835\n",
      "       I-PER       0.00      0.00      0.00      1156\n",
      "           O       0.83      0.91      0.87     38323\n",
      "\n",
      "    accuracy                           0.76     46435\n",
      "   macro avg       0.15      0.17      0.13     46435\n",
      "weighted avg       0.70      0.76      0.72     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Process text data for model training\n",
    "corpus_train = [item['words'].split() for item in training_features]\n",
    "\n",
    "# Initialize and train the Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=corpus_train, vector_size=50, window=3, min_count=2, workers=4, sample=0.00001)\n",
    "\n",
    "# Transform text to vectors using Word2Vec\n",
    "def embed(text, embedding_model):\n",
    "    embedding = np.zeros(embedding_model.vector_size)\n",
    "    words_counted = 0\n",
    "    for word in text.split():\n",
    "        if word in embedding_model.wv:\n",
    "            embedding += embedding_model.wv[word]\n",
    "            words_counted += 1\n",
    "    return embedding / words_counted if words_counted > 0 else embedding\n",
    "\n",
    "# Apply vectorization to training and testing datasets\n",
    "vectorized_corpus_train = np.array([embed(' '.join(words), w2v_model) for words in corpus_train])\n",
    "corpus_test = [datum['words'].split() for datum in test_features]\n",
    "vectorized_corpus_test = np.array([embed(' '.join(words), w2v_model) for words in corpus_test])\n",
    "\n",
    "# Convert labels into numeric form\n",
    "encoder = LabelEncoder()\n",
    "encoded_train_labels = encoder.fit_transform(training_gold_labels)\n",
    "encoded_test_labels = encoder.transform(test_gold_labels)\n",
    "\n",
    "# Set up and train the LinearSVC model\n",
    "svm_classifier = LinearSVC(class_weight='balanced')\n",
    "svm_classifier.fit(vectorized_corpus_train, encoded_train_labels)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "predictions = svm_classifier.predict(vectorized_corpus_test)\n",
    "evaluation_report = classification_report(encoded_test_labels, predictions, target_names=encoder.classes_)\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the RandomForestClassifier performs better than the other model, which tends to mistakenly label a variety of inputs as just \"O.\" \n",
    "\n",
    "While the first model incorrectly labels too many things as \"O,\" which results in high recall but low precision, the RandomForestClassifier strikes a more sensible balance, showcasing its improved ability to discriminate between labels.\n",
    "\n",
    "The RandomForestClassifier exhibits noticeably superior F1-scores than the original model, which demonstrated poor performance across multiple categories as evidenced by its zero F1-scores. This suggests that the RandomForestClassifier is highly capable of accurately identifying and classifying distinct items.\n",
    "\n",
    "Moreover, the RandomForestClassifier demonstrates proficiency in handling the typical problem of an excessive quantity of \"O\" labels, indicating a superior comprehension and handling of the data, while the first model struggles with this imbalance.\n",
    "\n",
    "In order to remedy its shortcomings, it appears that the original model will require a thorough change in the future. On the other hand, the RandomForestClassifier seems to be headed in the right direction, and more improvement could improve on its already excellent performance.\n",
    "\n",
    "Therefore, the data indicates that the RandomForestClassifier, which deviates greatly from the less successful tactics seen in the original model, is a superior fit for the NER job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/3l2r907n45j385wwylpn8rym0000gn/T/ipykernel_2725/3098159858.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  kaggle_dataset = kaggle_dataset.fillna(method=\"ffill\")\n"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = 'ner_dataset.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, on_bad_lines=\"skip\", encoding='latin1')\n",
    "kaggle_dataset = kaggle_dataset.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame.head(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Preparing features and labels for training and testing data\n",
    "def prepare_features_labels(df):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        features.append({'word': row['Word'], 'pos': row['POS']})\n",
    "        labels.append(row['Tag'])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "train_features, train_labels = prepare_features_labels(df_train)\n",
    "test_features, test_labels = prepare_features_labels(df_test)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "combined_features = train_features + test_features  # Combine to ensure consistent feature set\n",
    "combined_features_vectorized = vectorizer.fit_transform(combined_features)\n",
    "\n",
    "n_train = len(train_features)\n",
    "vectorized_train_features = combined_features_vectorized[:n_train]\n",
    "vectorized_test_features = combined_features_vectorized[n_train:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         4\n",
      "       B-eve       0.00      0.00      0.00         0\n",
      "       B-geo       0.80      0.76      0.78       741\n",
      "       B-gpe       0.96      0.92      0.94       296\n",
      "       B-nat       1.00      0.50      0.67         8\n",
      "       B-org       0.64      0.51      0.57       397\n",
      "       B-per       0.81      0.53      0.64       333\n",
      "       B-tim       0.91      0.76      0.83       393\n",
      "       I-art       0.00      0.00      0.00         0\n",
      "       I-eve       0.00      0.00      0.00         0\n",
      "       I-geo       0.74      0.50      0.60       156\n",
      "       I-gpe       1.00      0.50      0.67         2\n",
      "       I-nat       0.80      1.00      0.89         4\n",
      "       I-org       0.65      0.44      0.53       321\n",
      "       I-per       0.42      0.90      0.57       319\n",
      "       I-tim       0.41      0.08      0.14       108\n",
      "           O       0.98      0.99      0.99     16918\n",
      "\n",
      "    accuracy                           0.94     20000\n",
      "   macro avg       0.60      0.49      0.52     20000\n",
      "weighted avg       0.95      0.94      0.94     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/stefaniaconte/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.LinearSVC()\n",
    "svm_model.fit(vectorized_train_features, train_labels)\n",
    "\n",
    "test_predictions = svm_model.predict(vectorized_test_features)\n",
    "\n",
    "#probably we need to remove some labels that we almost have no values for.\n",
    "evaluation_report = classification_report(test_labels, test_predictions)\n",
    "\n",
    "\n",
    "print(evaluation_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision for 'B-art' is 0.00, indicating that the model did not correctly predict any instances of this class. Similar results are seen for 'B-eve,' 'I-art,' and 'I-eve.' High precision values (e.g., 0.96 for 'B-gpe' and 0.98 for 'O') suggest accurate predictions for these classes.\n",
    "\n",
    "Recall for 'B-geo' (0.76) indicates that the model correctly identified 76% of the instances of this class. Similar results are observed for other classes. However, low recall in classes such as 'B-art,' 'B-eve,' 'I-art,' and 'I-eve' suggests the model struggled to identify instances for these categories.\n",
    "\n",
    "The F1-score, a balanced measure of precision and recall, shows high values (e.g., 0.94 for 'B-gpe' and 0.99 for 'O') indicating a good balance between precision and recall. Conversely, low F1-scores (e.g., 0.00 for 'B-art' and 'B-eve') suggest poor performance for these classes.\n",
    "\n",
    "Support, reflecting the number of actual occurrences in the dataset, indicates that classes with low support (e.g., 'B-eve' and 'I-eve') have fewer instances, making the evaluation less reliable for these categories.\n",
    "\n",
    "The overall accuracy of the model on the test data is 94%, signifying good performance overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
